聚焦爬虫:爬取页面中指定的页面内容。
    - 编码流程:
        - 指定url
        - 发起请求
        - 获取响应数据
        - 数据解析
        - 持久化存储

数据解析分类:
    - 正则
    - bs4
    - xpath

数据解析原理概述
    - 解析的局部的文本内容都会在标签之间或者标签对应的属性中进行存储
    - 进行指定标签的定位
    - 标签或者标签对应的属性中存储的数据值进行提取（解析）

<a class="data-postion" data-position="detail-page" href="/tupian-304474832.html" target="_blank">
                                                                <img alt="网站信头模板高清图片" title="网站信头模板图片下载" class="lazy" src="//img95.699pic.com/photo/30447/4832.jpg_wh300.jpg!/fh/300/quality/90" data-original="//img95.699pic.com/photo/30447/4832.jpg_wh300.jpg!/fh/300/quality/90" width="1000" height="1000" style="display: inline;">
                                                            </a>

</div>

ex = '<div class = 'lazy'>.*?<img src="(.*?)" >.*?</div>'

bs4进行数据解析
    - 数据解析的原理:
        - 1.标签定位
        - 2.提取标签，标签属性中存储的数据值
    - bs4数据解析的原理:
        - 1.实例化一个Beautifulsoup对象，并且将页面源码数据加载到该对象中
        - 2.通过调用BeautifulSoup对象中相关的属性或者方法进行标签定位和数据提取
    - 环境安装
        - pip install bs4
        - pip install lxml
    - 如何实例化BeautifulSoup对象
        - from bs4 import BeautifulSoup
        - 对象的实例化:
            - 1.将本地的html文档中的数据加载到该对象中
                fp = open('./test.html', 'r', encoding='utf-8')
                soup = BeautifulSoup(fp, 'lxml')
            - 2.将互联网上获取到的页面源码加载到该对象中
                page_text = response.text
                soup = BeautifulSoup(page_text, 'lxml')
        - 3.提供的用于数据解析的方法和属性:
            - soup.tagname返回的是html中第一次出现的tagName标签
            - soup.find():
                soup.find('tagName') 返回的是html中第一次出现的tagName标签
            -属性定位:
                -soup.find('div', class_/id/attr='song')
            -soup.find_all():
                -soup.findall('tagName'):返回符合要求的所有标签（列表）
        -select:
            - select('某种选择器（id，class，标签...选择器）'),返回的是一个列表
            - 层级选择器：
                - soup.select('.top-nav > ul > li > span')#单层级
                - soup.select('.top-nav > ul > li span')#多层级
        - 获取标签之间的文本数据：
            - soup.a.text/string/get_text()
            - text/get_text():可以获取某一个标签中的所有的文本内容
            - string：只可以获取该标签下面直系的文本内容
        - 获取标签之间的属性值：
            - soup.a['href']

xpath解析：最常用且最便捷高效的一种解析方式。通用性

    - xpath解析原理:
        - 1.实例化一个etree的对象，并且将被解析的页面源码数据加载到该对象中。
        - 2.调用etree对象中的xpath方法结合xpath表达式实现标签的定位和内容的捕获。
    - 环境的安装:
        - pip install lxml
    - 如何实例化一个etree对象:from  lxml import etree
        - 1.将本地的html文档中的源码数据加载带etree中：
            etree.parse(filePath)
        - 2.将从互联网上获取的源码数据加载到该对象中
            etree.HTML('page_text')
        - xpath('xpath表达式')
    - xpath表达式
        - /:表示从根节点开始定位.表示的是一个层级。
        - //：表示的是多个层级。可以表示从任意位置开始定位。
        - 属性定位：//div[@class = 'tagName'] tag[@attrName = 'name']
        - 索引定位：//div[@class="song"]/p[1]'
        #索引定位,从1开始
        - 如何取文本：
            - /text() 获取的是标签中直系的文本内容
            - //text() 获取的是标签中所有的文本内容
        - 如何取属性：
            - /@attrName    ==>img/@src
作业：
    爬取站长素材中免费建立模板 https://sc.chinaz.com/